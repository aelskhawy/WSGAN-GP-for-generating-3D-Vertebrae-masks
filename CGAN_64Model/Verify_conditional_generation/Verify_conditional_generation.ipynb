{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch import  nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py as h5\n",
    "from collections import OrderedDict\n",
    "from utils import make_hyparam_string, save_new_pickle, read_pickle, Save_Voxels, generateZ,plotVoxelVisdom\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from data_utils import plot_3d_mesh, VertDataset, ResizeTo\n",
    "import pathlib\n",
    "\n",
    "from utils import var_or_cuda, plot_losess\n",
    "from model import _G, _D\n",
    "from lr_sh import  MultiStepLR\n",
    "from visdom import Visdom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class _G(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_G, self).__init__()\n",
    "        self.cube_len = 64\n",
    "        self.z_size = 100\n",
    "        self.bias = False\n",
    "\n",
    "\n",
    "        padd = (0, 0, 0)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(self.z_size + 2, self.cube_len*8, kernel_size=4, stride=2, bias=self.bias, padding=padd),\n",
    "            torch.nn.BatchNorm3d(self.cube_len*8),\n",
    "            torch.nn.ReLU(inplace = True)\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(self.cube_len*8, self.cube_len*4, kernel_size=4, stride=2, bias=self.bias, padding=(1, 1, 1)),\n",
    "            torch.nn.BatchNorm3d(self.cube_len*4),\n",
    "            torch.nn.ReLU(inplace = True)\n",
    "            #torch.nn.LeakyReLU(self.self.leak_value)\n",
    "        )\n",
    "                \n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(self.cube_len*4, self.cube_len*2, kernel_size=4, stride=2, bias=self.bias, padding=(1, 1, 1)),\n",
    "            torch.nn.BatchNorm3d(self.cube_len*2),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(self.cube_len*2, self.cube_len, kernel_size=4, stride=2, bias=self.bias, padding=(1, 1, 1)),\n",
    "            torch.nn.BatchNorm3d(self.cube_len),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(self.cube_len*1, 1, kernel_size=4, stride=2, bias=self.bias, padding=(1, 1, 1)),\n",
    "            #torch.nn.BatchNorm3d(1), #lets try opening this later \n",
    "            #torch.nn.Linear(-1,1),\n",
    "            torch.nn.ReLU(),\n",
    "            #torch.nn.LeakyReLU(self.self.leak_value),\n",
    "            torch.nn.Tanh() ###\n",
    "            #torch.nn.Sigmoid()\n",
    "        )\n",
    "      \n",
    "\n",
    "    def forward(self, x, one_hot_labels):\n",
    "        # shape (batch size,2, 1,1,1)\n",
    "        one_hot_labels = one_hot_labels.unsqueeze(2).unsqueeze(3).unsqueeze(4)\n",
    "\n",
    "        out = x.view(-1, self.z_size, 1, 1, 1)\n",
    "        #print(\"start\",out.size()) # torch.Size([64, 100, 1, 1, 1])\n",
    "\n",
    "        # concatenate x (z noise vector) with the one hot labels\n",
    "        out = torch.cat([out, one_hot_labels], dim=1)\n",
    "        print(\"Concatination of noise vector and one hot labels\", out.size()) # torch.size([64,102,1,1,1])\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        #print(\"G:L1\",out.size())  #torch.Size([64, 512, 4, 4, 4])\n",
    "\n",
    "\n",
    "        out = self.layer2(out)\n",
    "        #print(\"G:L2\",out.size()) # torch.Size([64, 256, 8, 8, 8])\n",
    "        out = self.layer3(out)\n",
    "        #print(\"G:L3\",out.size()) # torch.Size([64, 128, 16, 16, 16])\n",
    "        out = self.layer4(out)\n",
    "        #print(\"G:L4\",out.size()) #  torch.Size([64, 64, 32, 32, 32])\n",
    "        out = self.layer5(out)\n",
    "        #print(\"G:L5\",out.size()) #  torch.Size([64, 1, 64, 64, 64])\n",
    "        out = out.view(-1, self.cube_len*self.cube_len*self.cube_len)\n",
    "        #print(\"G:Final\",out.size()) # torch.Size([64, 32768])\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class _D(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_D, self).__init__()\n",
    "        self.cube_len = 64\n",
    "        self.bias = False\n",
    "        self.leak_value = 0.2\n",
    "\n",
    "        padd = (0,0,0)\n",
    "        if self.cube_len == 32:\n",
    "            padd = (1,1,1)\n",
    "        # self.process_labels = torch.nn.Sequential(\n",
    "        #     torch.nn.Conv3d(2, self.cube_len, kernel_size=4, stride=2, bias=self.bias, padding=(1, 1, 1) ),\n",
    "        #     torch.nn.LeakyReLU(0.2)\n",
    "        # )\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(1+2, self.cube_len, kernel_size=4, stride=2, bias=self.bias, padding=(1, 1, 1)),\n",
    "            torch.nn.LayerNorm([64, 32, 32, 32]),\n",
    "            torch.nn.LeakyReLU(self.leak_value)\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(self.cube_len, self.cube_len*2, kernel_size=4, stride=2, bias=self.bias, padding=(1, 1, 1)),\n",
    "\t    torch.nn.LayerNorm([128, 16, 16, 16]),\n",
    "            torch.nn.LeakyReLU(self.leak_value)\n",
    "        )\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(self.cube_len*2, self.cube_len*4, kernel_size=4, stride=2, bias=self.bias, padding=(1, 1, 1)),\n",
    "            torch.nn.LayerNorm([256, 8, 8, 8]),\n",
    "            torch.nn.LeakyReLU(self.leak_value)\n",
    "        )\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(self.cube_len*4, self.cube_len*8, kernel_size=4, stride=2, bias=self.bias, padding=(1, 1, 1)),\n",
    "            torch.nn.LayerNorm([512, 4, 4, 4]),\n",
    "            torch.nn.LeakyReLU(self.leak_value)\n",
    "        )\n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(self.cube_len*8, 1, kernel_size=4, stride=2, bias=self.bias, padding=(0,0,0)),\n",
    "            #torch.nn.LeakyReLU(self.leak_value)\n",
    "            #torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, one_hot_labels):\n",
    "        # shape (batch, 2, 1,1,1)\n",
    "        one_hot_labels = one_hot_labels.unsqueeze(2).unsqueeze(3).unsqueeze(4)\n",
    "        one_hot_labels = one_hot_labels.repeat(1,1,64,64,64)\n",
    "        #print(\"One hot labels in D \",one_hot_labels.shape)\n",
    "\n",
    "        out = x.view(-1, 1, self.cube_len, self.cube_len, self.cube_len)\n",
    "        #print(\"start D\",out.size()) #  torch.Size([64, 1, 64, 64, 64])\n",
    "\n",
    "        # concatenate processed labels and x\n",
    "        out = torch.cat([out, one_hot_labels], dim=1)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        #print(\"D:L1\",out.size()) #  torch.Size([64, 64, 32, 32, 32])\n",
    "\n",
    "\n",
    "\n",
    "        out = self.layer2(out)\n",
    "        #print(\"D:L2\",out.size()) #  torch.Size([64, 128, 16, 16, 16])\n",
    "        out = self.layer3(out)\n",
    "        #print(\"D:L3\",out.size()) #  torch.Size([64, 256, 8, 8, 8])\n",
    "        out = self.layer4(out)\n",
    "        #print(\"D:L4\",out.size()) #  torch.Size([64, 512, 4, 4, 4])\n",
    "        out = self.layer5(out)\n",
    "        #print(\"D:L5\",out.size()) #  torch.Size([64, 1, 1, 1, 1])\n",
    "        out = out.view(-1,1) \n",
    "        #print(\"final\",out.size()) # torch.Size([64, 1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n",
      "905 ./pickled_model\n",
      "Done loading G 905\n",
      "Done loading G_Opti 905\n",
      "Done loading D 905\n",
      "Done loading D_Optim 905\n"
     ]
    }
   ],
   "source": [
    "#  Build the model\n",
    "D = _D()\n",
    "G = _G()\n",
    "d_lr = 1e-2\n",
    "g_lr = 0.00025\n",
    "beta = (0.9, 0.99)\n",
    "#Create the solvers\n",
    "D_solver = optim.Adam(D.parameters(), lr=d_lr, betas=beta)\n",
    "G_solver = optim.Adam(G.parameters(), lr=g_lr, betas=beta)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"using cuda\")\n",
    "    D.cuda()\n",
    "    G.cuda()\n",
    "\n",
    "pickle_path = \"./pickled_model\"\n",
    "#Load checkpoint if available\n",
    "read_pickle(pickle_path, G, G_solver, D, D_solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "Concatination of noise vector and one hot labels torch.Size([1, 102, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "z1 = generateZ(1,z_size=100)\n",
    "# label 1 ([0,1]) lumbar, 0 ([1,0]) thoracic \n",
    "label = torch.Tensor(np.asarray([1,0])).cuda().unsqueeze(0) \n",
    "print(label.shape)\n",
    "gout1 = G(z1,label)\n",
    "samples = gout1.cpu().data.squeeze().numpy()\n",
    "samples = samples.reshape(-1,64,64,64)\n",
    "Save_Voxels(samples, './ls', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visdom import Visdom\n",
    "import pickle\n",
    "import skimage.measure as sk\n",
    "from skimage.measure import marching_cubes_lewiner\n",
    "import h5py as h5\n",
    "import numpy as np \n",
    "from data_utils import plot_3d_mesh\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVFByMarchingCubes(voxels, threshold=0.5):\n",
    "    \"\"\"Voxel 로 부터 Vertices, faces 리턴 하는 함수\"\"\"\n",
    "    #v, f = sk.marching_cubes_classic(voxels) #, level=threshold)\n",
    "    #voxels = np.pad(voxels, pad_width=((1, 1), (1, 1), (1, 1)), mode='constant')\n",
    "    v, f, _, _  = marching_cubes_lewiner(voxels, level= threshold)#\n",
    "    return v, f\n",
    "\n",
    "\n",
    "def plotVoxelVisdom(voxels, visdom, title):\n",
    "    v, f = getVFByMarchingCubes(voxels)\n",
    "    visdom.mesh(X=v, Y=f, opts=dict(opacity=0.5, title=title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PORT = 8097\n",
    "DEFAULT_HOSTNAME = \"http://localhost\"\n",
    "viz = Visdom(DEFAULT_HOSTNAME,DEFAULT_PORT, ipv6=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "number= '001'\n",
    "filename=\"./ls/\" +str(number) + \".pkl\"\n",
    "with open(filename, \"rb\") as f:\n",
    "    voxels = pickle.load(f).squeeze()\n",
    "    #plot_3d_mesh(voxels)\n",
    "    plotVoxelVisdom(voxels, viz, \"0-Thoracic\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
